// BEGIN -- inclusion -- page/partial -- {root-commons}query-troubleshooting.adoc
// USED-IN: <source-language>-troubleshooting.adoc
// Applies-to: All platforms, except Javascript

// DO NOT EDIT
ifdef::param-fullpage[]
// Only include this abstract if the inclusion is used as a full page, rather than as a component
// Allow for over-riding of default abstract parameter values
:tslinks: {xref-cbl-pg-troubleshooting-logs}
ifeval::["{param-platform}"=="{platform-ios}"]
:tslinks: {tslinks} | {xref-cbl-pg-troubleshooting-crashes}
endif::[]
ifndef::param-abstract[]
:param-abstract: This content describes how to use the Couchbase Lite on {param-title} Query API's explain() method to examine a query
endif::[]
ifndef::param-related[]
:param-related:  {tslinks} | {xref-cbl-pg-query-predictive} | {xref-cbl-pg-query-live} | {xref-cbl-pg-indexing}
endif::[]
ifndef::topic-group[]
:topic-group: Troubleshooting
endif::[]
include::{root-partials}_block-abstract.adoc[]
endif::param-fullpage[]
// DO NOT EDIT
:url-sql-query-plan: https://www.sqlite.org/eqp.html

== Query Explain
=== Using
Query's {url-api-method-query-explain} method can provide useful insight when you are trying to diagnose query performance issues and-or optimize queries.
You can embed the call inside your app, or use it interactively within a `cblite` shell, to examine how your query is working -- see: <<use-qe>> for how to use it in either scenario.

[#use-qe]
.Using Query Explain
=====
[{tabs}]
====
In App::
+
--
[source, {source-language}]
----
include::{snippet}[tag="query-explain-all", indent=0]
----
<.> Construct your query as normal
<.> Call the query's `explain` method; All output is sent to the application's log file.
--

In cblite::
+
--
[source, console]
----
cblite <your-database-name>.cblite2 // <.>

(cblite) select --explain domains group by country order by country, name // <.>

(cblite) query --explain {"GROUP_BY":[[".country"]],"ORDER_BY":[[".country"],[".name"]],"WHAT":[[".domains"]]} // <.>

----
<.> Within a terminal session open your database with `cblite` and enter your query
<.> Here the query is entered as a N1QL-query using `select` +
<.> Here the query is entered as a JSON-string using `query`

--
====
=====

=== Output
The output from `{url-api-method-query-explain}` remains the same whether invoked by an app, or `cblite` -- see <<qe-output>> for an example of how it looks.

[#qe-output]
.Query.explain() Output
====

[source, console]
----
SELECT fl_result(fl_value(_doc.body, 'domains')) FROM kv_default AS _doc WHERE (_doc.flags & 1 = 0) GROUP BY fl_value(_doc.body, 'country') ORDER BY fl_value(_doc.body, 'country'), fl_value(_doc.body, 'name') // <.>

7|0|0| SCAN TABLE kv_default AS _doc // <.>
12|0|0| USE TEMP B-TREE FOR GROUP BY
52|0|0| USE TEMP B-TREE FOR ORDER BY

{"GROUP_BY":[[".country"]],"ORDER_BY":[[".country"],[".name"]],"WHAT":[[".domains"]]} // <.>

----

====

This output (<<qe-output>>) comprises three main elements:

<.> The translated SQL-query, which is not necessarily useful, being aimed more at Couchbase support and-or engineering teams.
<.> The _SQLite_ query plan, which gives a high-level view of how the SQL query will be implemented.
You can use this to identify potential issues and so optimize problematic queries.
<.> The query in JSON-string format, which you can copy-and-paste directly into the _cblite_ tool.

== The Query Plan
=== Format
The query plan section of the output displays a tabular form of the translated query's execution plan.
It primarily shows how the data will be retrieved and, where appropriate, how it will be sorted for navigation and-or presentation purposes.
For more on SQLite's Explain Query Plan -- see: {url-sql-query-plan}

[#qry-plan]
.A Query Plan
====
[source, console]
----
7|0|0| SCAN TABLE kv_default AS _doc // <.>
12|0|0| USE TEMP B-TREE FOR GROUP BY // <.>
52|0|0| USE TEMP B-TREE FOR ORDER BY // <.>
----

<.> *Retrieval method* -- This line shows the retrieval method being used for the query; here a sequential read of the database.
Something you may well be looking to optimize -- see <<ret-method>> for more.
<.> *Grouping method* --- This line shows that the *Group By* clause used in the query requires the data to be sorted and that a b-tree will be used for temporary storage -- see <<order-group>>.
<.> *Ordering method* -- This line shows that the *Order By* clause used in the query requires the data to be sorted and that a b-tree will be used for temporary storage -- see <<order-group>>.
====


[#ret-method]
=== Retrieval Method
The query optimizer will attempt to retrieve the requested data items as efficiently as possible, which generally will be by using one or more of the available indexes.
The _retrieval method_ shows the approach decided upon by the optimizer -- see <<ret-meths>>.

[#ret-meths]
.Retrieval methods
[#,cols="2,8"]
|===
|Retrieval Method | Description

|Search
|Here the query is able to access the required data directly using keys into the index.
Queries using the Search mode are the fastest.

|Scan Index
|Here the query is able to retrieve the data by scanning all or part-of the index (for example when seeking to match values within a range).
This type of query is slower than search, but at least benefits from the compact and ordered form of the index.

|Scan Table
|Here the query must scan the database table(s) to retrieve the required data.
It is the slowest of these methods and will benefit most from some form of optimization.
|===

When looking to optimize a query's retrieval method, consider whether:

* Providing an additional index makes sense
* You could use an existing index -- perhaps by restructuring the query to minimize wildcard use, or the reliance on functions that modify the query's interpretation of index keys (for example, 'lower')
* You could reduce the data set being requested to minimize the query's footprint on the database

[#order-group]
=== Order and Group
The `Use temp b-tree for` lines in the example indicate that the query requires sorting to cater for grouping and then sorting again to present the output results.
Minimizing, if not eliminating, this ordering and re-ordering will obviously reduce the amount of time taken to process your query.

Ask "is the grouping and-or ordering absolutely necessary?": if it isn't, drop it or modify it to minimize its impact.

== Queries and Indexes
include::{root-commons}indexing.adoc[tag=overview]

The Query optimizer converts your query into a parse tree that groups zero or more _and-connected_ clauses together (as dictated by your `where` conditionals) for effective query engine processing.

Ideally a query will be be able to satisfy its requirements entirely by either directly accessing the index or searching sequential index rows.
Less good is if the query must scan the whole index; although the compact nature of most indexes means this is still much faster than the alternative of scanning the entire database with no help from the indexes at all.

Searches that begin with or rely upon an inequality with the primary key are inherently less effective than those using a primary key equality.

[#use-like-based-queries]
== Wildcard and Like-based Queries

Like-based searches can use the index(es) only if:

* The search-string doesn't start with a wildcard
* The primary search expression uses a property that is indexed key
* The search-string is a constant known at run time) (that is, not a value derived during processing of the query)

To illustrate this we can use a modified query from the Mobile Travel Sample application; replacing a simple equality test with a 'LIKE'

In the first example (<<like-wild-pfx-qry>>) we use a wildcard prefix and suffix.
You can see that the query plan decides on a retrieval method of `Scan Table`.

For more on indexes -- see: {xref-cbl-pg-indexing}

[#like-wild-pfx-qry]
.Like with Wildcard Prefix
====
[source, {source-language}]
----
include::{snippet}[tag="query-explain-like", indent=0]

----

<.> The indexed property, TYPE, cannot use its index because of the wildcard prefix.

.Resulting Query Plan
[source, console]
----
2|0|0| SCAN TABLE kv_default AS _doc
----

====

By contrast, by removing the wildcard prefix `%` (in <<like-no-wild-pfx-qry>>), we see that the query plan's retrieval method changes to become an index search.
Where practical, simple changes like this can make significant differences in query performance.

[#like-no-wild-pfx-qry]
.Like with No Wildcard-prefix
====
[source, {source-language}]
----
include::{snippet}[tag="query-explain-nopfx", indent=0]

----

<.> Simply removing the wildcard prefix enables the query optimizer to access the `typeIndex`, which results in a more efficient search.

.Resulting Query Plan
[source, {console}]
----
3|0|0| SEARCH TABLE kv_default AS _doc USING INDEX typeIndex (<expr>>? AND <expr><?)
----
====

== Use Functions Wisely

Functions are a very useful tool in building queries, but be aware that they can impact whether the query-optimizer is able to use your index(es).

For example, you can observe a similar situation to that shown in <<use-like-based-queries>> when using the `{url-api-method-function-lower}` function on an indexed property.

.Query
[source, {source-language}]
----
include::{snippet}[tag="query-explain-function", indent=0]

----
<.> Here we use the `{url-api-method-function-lower}` function in the _Where_ expression

.Query Plan:
[source, {console}]
----
2|0|0| SCAN TABLE kv_default AS _doc
----

// END: common-query-troubleshooting.adoc

But removing the `{url-api-method-function-lower}` function, changes things:

.Query
----
include::{snippet}[tag="query-explain-nofunction", indent=0]

----
<.> Here we have removed `{url-api-method-function-lower}` from the _Where_ expression

.Query plan
----
3|0|0| SEARCH TABLE kv_default AS _doc USING INDEX typeIndex (<expr>=?)
----

Knowing this, you can consider how you create the index; for example, using {url-api-method-function-lower} when you create the index and then always using lowercase comparisons.

== Optimization Considerations
Try to minimize the amount of data retrieved.
Reduce it down to the few properties you really *do* need to achieve the required result.

Consider fetching details _lazily_.
You could break complex queries into components.
Returning just the doc-ids, then process the array of doc-ids using either the Document API or a query thats uses the array of doc-ids to return information.

Consider using paging to minimize the data returned when the number of results returned is expected to be high.
Getting the whole lot at once will be slow and resource intensive: Plus does anyone want to access them all in one go?
Instead retrieve batches of information at a time, perhaps using `Where` method's `limit( offset)` feature to set a starting point for each batch subsequent batch.
Although, note that using query offsets becomes increasingly less effective as the overhead of skipping a growing number of rows each time increases. You can work around this, by instead using ranges of search-key values. If the last search-key value of batch one was 'x' then that could become the starting point for your next batch and-so-on.

Optimize document size in design.
Smaller docs load more quickly.
Break your data into logical linked units.

Consider Using Full Text Search instead of complex like or regex patterns -- see {xref-cbl-pg-fts}.

ifdef::param-fullpage[]
include::{root-partials}block-related-content-query.adoc[]
endif::[]

// void used attributes and locals
:param-fullpage!:
:tslinks!:
// End; inclusion